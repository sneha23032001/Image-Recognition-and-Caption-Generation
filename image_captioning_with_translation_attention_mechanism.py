# -*- coding: utf-8 -*-
"""Image_Captioning_with_Translation_Attention Mechanism.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LIYQFiQJeWu3kmuoU7MQAbevJCqe2Ldv

# **Mounting the data on drive**
"""

from google.colab import drive
drive.mount('/content/drive/')

"""# **Importing the libraries**"""

import string
import numpy as np
import pandas as pd
from numpy import array
from pickle import load
from PIL import Image
import pickle
from collections import Counter
import matplotlib.pyplot as plt
import sys, time, os, warnings
warnings.filterwarnings("ignore")
import re
import keras
import tensorflow as tf
from tqdm import tqdm
from nltk.translate.bleu_score import sentence_bleu
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical, plot_model
from keras.models import Model
from keras.layers import Input, Dense, BatchNormalization, LSTM, Embedding, Dropout
from keras.layers.merge import add
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Dropout, GRU, Embedding, BatchNormalization
from tensorflow.compat.v1.keras.layers import CuDNNLSTM
from tensorflow.nn import relu, tanh, softmax
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.data.experimental import AUTOTUNE
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
!pip install translate
from translate import Translator
!pip install -U nltk
from nltk.translate.meteor_score import meteor_score
import nltk
nltk.download('wordnet')
!pip install rouge/requirements.txt
!pip install rouge-score
from rouge_score import rouge_scorer
# from pycocoevalcap.cider.cider import Cider

"""# **Data loading and Preprocessing**"""

image_path = "/content/drive/My Drive/Image Captioning with Translation/Flickr8k_dataset/Images/"
caption_file = "/content/drive/My Drive/Image Captioning with Translation/Flickr8k_dataset/Captions/captions.txt"
images_jpg = os.listdir(image_path)

print("Total Images = {}".format(len(images_jpg)))

# Read data from captions.txt
file = open(caption_file,'r')
text = file.read()
file.close()

text

# Saving captions file dataset in list
datatxt = []
for line in text.split('\n'):
   col = line.split('\t')
   if len(col) == 1:
       continue
   w = col[0].split("#")
   datatxt.append(w + [col[1].lower()])

datatxt

# Creating Dataframe of captions and imagename 
data = pd.DataFrame(datatxt,columns=["ImageName","index","Caption"])
data = data.reindex(columns =['index','ImageName','Caption'])
data = data[data.ImageName != '2258277193_586949ec62.jpg.1']
unique_file = np.unique(data.ImageName.values)
data.head()

"""# **Visualising the dataset**"""

count = 1

fig = plt.figure(figsize=(10,20))
for jpgName in unique_file[10:16]:
   ImageName = image_path + '/' + jpgName
   captions = list(data["Caption"].loc[data["ImageName"]==jpgName].values)
   image_load = load_img(ImageName, target_size=(224,224,3))
   ax = fig.add_subplot(10,2,count,xticks=[],yticks=[])
   ax.imshow(image_load)
   count += 1
   ax = fig.add_subplot(10,2,count)
   plt.axis('off')
   ax.plot()
   ax.set_xlim(0,1)
   ax.set_ylim(0,len(captions))
   for i, caption in enumerate(captions):
       ax.text(0,i,caption,fontsize=20)
   count += 1
plt.show()

"""# **Data Wrangling**"""

for i, caption in enumerate(data.Caption.values):
   text_no_punctuation = caption.translate(string.punctuation) # removing punctuations from each token

   text_len_more_than1 = ""
   for word in text_no_punctuation.split(): # removing single character words
       if len(word) > 1:
           text_len_more_than1 += " " + word

   text_no_numeric = ""
   for word in text_len_more_than1.split(): # remove tokens with numbers in them
       isalpha = word.isalpha()
       if isalpha:
           text_no_numeric += " " + word 

   text = text_no_numeric       
   data["Caption"].iloc[i] = text

# Clean vocabulary after cleaning
vocabulary = [] 
for txt in data.Caption.values:
   vocabulary.extend(txt.split())
print('Vocabulary Size: %d' % len(set(vocabulary)))

# Add ‘< start >’ and ‘< end >’ tags to every caption
path = "/content/drive/My Drive/Image Captioning with Translation/Flickr8k_dataset/Images/"
captions = []
for caption in data["Caption"].astype(str):
   caption = '<start> ' + caption+ ' <end>'
   captions.append(caption)

print(f"len(captions) : {len(captions)}")

img_name_path = []
for url in data["ImageName"]:
   full_image_path = path + url
   img_name_path.append(full_image_path)

print(f"len(img_name_path) : {len(img_name_path)}")

# we define a function to limit the dataset to 40000 images and captions
def data_limiter(num,total_captions,img_name_path):
 train_captions, img_name_vector = shuffle(total_captions,img_name_path,random_state=1)
 train_captions = train_captions[:num]
 img_name_vector = img_name_vector[:num]
 return train_captions,img_name_vector

train_captions,img_name_vector = data_limiter(40000,captions,img_name_path)

"""# **Image feature extraction model using ResNet50**"""

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Image Captioning with Translation"

model = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')
model = tf.keras.Model(model.input, model.layers[-1].output)
model.summary()

checkpoint_path = "ResNet50.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)

def load_image(image_path):
   img = tf.io.read_file(image_path)
   img = tf.image.decode_jpeg(img, channels=3)
   img = tf.image.resize(img, (224, 224))
   img = tf.keras.applications.resnet50.preprocess_input(img)
   return img, image_path

# Map each image name to the function to load the image
encode_train = sorted(set(img_name_vector))
image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)
image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for img, path in tqdm(image_dataset):
#  batch_features = model(img)
#  batch_features = tf.reshape(batch_features,
#                              (batch_features.shape[0], -1, batch_features.shape[3]))
# 
#  for bf, p in zip(batch_features, path):
#    path_of_feature = p.numpy().decode("utf-8")
#    np.save(path_of_feature, bf.numpy())

"""# **Tokenize the vocabulary**"""

tokenizer = Tokenizer(oov_token="<unk>", filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ') # replace words not in vocabulary with the token < unk >
tokenizer.fit_on_texts(train_captions)
tokenizer.word_index['<pad>'] = 0
tokenizer.index_word[0] = '<pad>'
vocab_size = len(tokenizer.word_index) + 1

train_seqs = tokenizer.texts_to_sequences(train_captions)
cap_vector = pad_sequences(train_seqs, padding='post')

max_length = max(len(t) for t in train_seqs)
min_length = min(len(t) for t in train_seqs)
print('Max Length of any caption = '+ str(max_length))
print('Min Length of any caption = '+ str(min_length))

"""# **Attention Mechanism**"""

img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.2, random_state=0)

# parameters for training:
units = 512
vocab_size = len(tokenizer.word_index) + 1
num_steps = len(img_name_train) // 64
features_shape = 512
attention_features_shape = 49

def map_func(img_name, cap):
 img_tensor = np.load(img_name.decode('utf-8')+'.npy')
 return img_tensor, cap
 
dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))
dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=AUTOTUNE)
dataset = dataset.shuffle(1000).batch(64)
dataset = dataset.prefetch(buffer_size=AUTOTUNE)

#CNN_Encoder
class ResNet50_Encoder(Model): 
   # This encoder passes the features through a Fully connected layer
   def __init__(self):
       super(ResNet50_Encoder, self).__init__()
       self.fc = Dense(256)
       self.dropout = Dropout(0.5, noise_shape=None, seed=None)

   def call(self, x):
       x = self.fc(x)
       x = relu(x)
       return x

# RNN based on GPU/CPU capabilities
def rnn_type(units):
   if tf.config.list_physical_devices('GPU'):
       return CuDNNLSTM(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')
   else:
       return GRU(units, return_sequences=True, return_state=True, recurrent_activation='sigmoid', recurrent_initializer='glorot_uniform')

# RNN Decoder with Bahdanau Attention
class Rnn_Local_Decoder_GRU(Model): 
 def __init__(self, units, vocab_size):
   super(Rnn_Local_Decoder_GRU, self).__init__()
   self.units = units
   self.embedding = Embedding(vocab_size, 256)
   self.gru = GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')
   self.fc1 = Dense(self.units)
   self.dropout = Dropout(0.5, noise_shape=None, seed=None)
   self.batchnormalization = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)
   self.fc2 = Dense(vocab_size)

   # Implementing Bahdanau Attention Mechanism
   self.Uattn = Dense(units)
   self.Wattn = Dense(units)
   self.Vattn = Dense(1)

 def call(self, x, features, hidden):
   hidden_with_time_axis = tf.expand_dims(hidden, 1)
   # Attention Function
   score = self.Vattn(tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))
   # find Probability using Softmax
   attention_weights = softmax(score, axis=1)
   # Give weights to the different pixels in the image
   context_vector = attention_weights * features
   context_vector = tf.reduce_sum(context_vector, axis=1)

   x = self.embedding(x)
   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
   # passing the concatenated vector to the GRU
   output, state = self.gru(x)
   x = self.fc1(output)
   x = tf.reshape(x, (-1, x.shape[2]))
   # Adding Dropout and BatchNorm Layers
   x= self.dropout(x)
   x= self.batchnormalization(x)
   x = self.fc2(x)

   return x, state, attention_weights

 def reset_state(self, batch_size):
   return tf.zeros((batch_size, self.units))


encoder = ResNet50_Encoder()
decoder = Rnn_Local_Decoder_GRU(units, vocab_size)

optimizer = Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')

def loss_function(real, pred):
 mask = tf.math.logical_not(tf.math.equal(real, 0))
 loss_ = loss_object(real, pred)
 mask = tf.cast(mask, dtype=loss_.dtype)
 loss_ *= mask

 return tf.reduce_mean(loss_)

"""# **Model Training**"""

# Teacher Forcing
loss_plot = []

@tf.function
def train_step(img_tensor, target):
 loss = 0
 # initializing the hidden state for each batch because the captions are not related from image to image
 hidden = decoder.reset_state(batch_size=target.shape[0])
 dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * 64, 1)

 with tf.GradientTape() as tape:
     features = encoder(img_tensor)
     for i in range(1, target.shape[1]):
         # passing the features through the decoder
         predictions, hidden, _ = decoder(dec_input, features, hidden)
         loss += loss_function(target[:, i], predictions)
         # using teacher forcing
         dec_input = tf.expand_dims(target[:, i], 1)

 total_loss = (loss / int(target.shape[1]))
 trainable_variables = encoder.trainable_variables + decoder.trainable_variables
 gradients = tape.gradient(loss, trainable_variables)
 optimizer.apply_gradients(zip(gradients, trainable_variables))

 return loss, total_loss

EPOCHS = 30
start_epoch = 0
for epoch in range(start_epoch, EPOCHS):
   start = time.time()
   total_loss = 0

   for (batch, (img_tensor, target)) in enumerate(dataset):
       batch_loss, t_loss = train_step(img_tensor, target)
       total_loss += t_loss

       if batch % 100 == 0:
           print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))
   # storing the epoch end loss value to plot later
   loss_plot.append(total_loss / num_steps)

   print ('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss/num_steps))
   print ('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

plt.figure(figsize=(16, 10))
plt.plot(loss_plot)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Plot')
plt.show()

"""# **Greedy Search**"""

def evaluate(image):
   attention_plot = np.zeros((max_length, attention_features_shape))

   hidden = decoder.reset_state(batch_size=1)
   temp_input = tf.expand_dims(load_image(image)[0], 0)
   img_tensor_val = model(temp_input)
   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))

   features = encoder(img_tensor_val)
   dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)
   result = []

   for i in range(max_length):
       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)
       attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()
       predicted_id = tf.argmax(predictions[0]).numpy()
       result.append(tokenizer.index_word[predicted_id])

       if tokenizer.index_word[predicted_id] == '<end>':
           return result, attention_plot

       dec_input = tf.expand_dims([predicted_id], 0)
   attention_plot = attention_plot[:len(result), :]

   return result, attention_plot

def plot_attention(image, result, attention_plot):
   temp_image = np.array(Image.open(image))
   fig = plt.figure(figsize=(10, 10))
   len_result = len(result)
   for l in range(len_result):
       temp_att = np.resize(attention_plot[l], (8, 8))
       ax = fig.add_subplot(len_result//2, len_result//2, l+1)
       ax.set_title(result[l])
       img = ax.imshow(temp_image)
       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())

   plt.tight_layout()
   plt.show()

"""# **Vaidation with BLEU, METEOR and ROUGE-L Evaluation**"""

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]

real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

# remove <start> and <end> from the real_caption
first = real_caption.split(' ', 1)[1]
real_caption = first.rsplit(' ', 1)[0]

#remove "<unk>" in result
for i in result:
   if i=="<unk>":
       result.remove(i)

for i in real_caption:
   if i=="<unk>":
       real_caption.remove(i)

#remove <end> from result        
result_join = ' '.join(result)
result_final = result_join.rsplit(' ', 1)[0]

real_appn = []
real_appn.append(real_caption.split())
reference = real_appn
candidate = result

# cider_score = {}
# score, scores = Cider().compute_score(reference, candidate)
# cider_score["CIDEr"] = score

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
r_score = scorer.score(real_caption,result_final)
  
score = sentence_bleu(reference, candidate)
meteor = meteor_score(reference, candidate)
print(f"METEOR score: {meteor*100}")
print(f"ROUGE-L score: {r_score}")
print(f"BLEU score: {score*100}")
# print(f"CIDEr score: {cider_score}")

print('BLEU-1 score: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('BLEU-2 score: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('BLEU-3 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('BLEU-4 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))

print ('Real Caption:', real_caption)
print ('Prediction Caption:', result_final)
plot_attention(image, result, attention_plot)
print(f"time took to Predict: {round(time.time()-start)} sec")

Image.open(img_name_val[rid])

"""# **Caption Translation**"""

to_lang = input("Enter a language to translate the caption : ")
print(to_lang)

translator= Translator(to_lang=to_lang)
translation = translator.translate(result_final)
print("Translated text : ")
print(translation)

"""# **Example Predictions**"""

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]

real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

# remove <start> and <end> from the real_caption
first = real_caption.split(' ', 1)[1]
real_caption = first.rsplit(' ', 1)[0]

#remove "<unk>" in result
for i in result:
   if i=="<unk>":
       result.remove(i)

for i in real_caption:
   if i=="<unk>":
       real_caption.remove(i)

#remove <end> from result        
result_join = ' '.join(result)
result_final = result_join.rsplit(' ', 1)[0]

real_appn = []
real_appn.append(real_caption.split())
reference = real_appn
candidate = result

# cider_score = {}
# score, scores = Cider().compute_score(reference, candidate)
# cider_score["CIDEr"] = score

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
r_score = scorer.score(real_caption,result_final)
  
score = sentence_bleu(reference, candidate)
meteor = meteor_score(reference, candidate)
print(f"METEOR score: {meteor*100}")
print(f"ROUGE-L score: {r_score}")
print(f"BLEU score: {score*100}")
# print(f"CIDEr score: {cider_score}")

print('BLEU-1 score: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('BLEU-2 score: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('BLEU-3 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('BLEU-4 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))

print ('Real Caption:', real_caption)
print ('Prediction Caption:', result_final)
plot_attention(image, result, attention_plot)
print(f"time took to Predict: {round(time.time()-start)} sec")

Image.open(img_name_val[rid])

to_lang = input("Enter a language to translate the caption : ")
print(to_lang)

translator= Translator(to_lang=to_lang)
translation = translator.translate(result_final)
print("Translated text : ")
print(translation)

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]

real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

# remove <start> and <end> from the real_caption
first = real_caption.split(' ', 1)[1]
real_caption = first.rsplit(' ', 1)[0]

#remove "<unk>" in result
for i in result:
   if i=="<unk>":
       result.remove(i)

for i in real_caption:
   if i=="<unk>":
       real_caption.remove(i)

#remove <end> from result        
result_join = ' '.join(result)
result_final = result_join.rsplit(' ', 1)[0]

real_appn = []
real_appn.append(real_caption.split())
reference = real_appn
candidate = result

# cider_score = {}
# score, scores = Cider().compute_score(reference, candidate)
# cider_score["CIDEr"] = score

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
r_score = scorer.score(real_caption,result_final)
  
score = sentence_bleu(reference, candidate)
meteor = meteor_score(reference, candidate)
print(f"METEOR score: {meteor*100}")
print(f"ROUGE-L score: {r_score}")
print(f"BLEU score: {score*100}")
# print(f"CIDEr score: {cider_score}")

print('BLEU-1 score: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('BLEU-2 score: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('BLEU-3 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('BLEU-4 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))

print ('Real Caption:', real_caption)
print ('Prediction Caption:', result_final)
plot_attention(image, result, attention_plot)
print(f"time took to Predict: {round(time.time()-start)} sec")

Image.open(img_name_val[rid])

to_lang = input("Enter a language to translate the caption : ")
print(to_lang)

translator= Translator(to_lang=to_lang)
translation = translator.translate(result_final)
print("Translated text : ")
print(translation)

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]

real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

# remove <start> and <end> from the real_caption
first = real_caption.split(' ', 1)[1]
real_caption = first.rsplit(' ', 1)[0]

#remove "<unk>" in result
for i in result:
   if i=="<unk>":
       result.remove(i)

for i in real_caption:
   if i=="<unk>":
       real_caption.remove(i)

#remove <end> from result        
result_join = ' '.join(result)
result_final = result_join.rsplit(' ', 1)[0]

real_appn = []
real_appn.append(real_caption.split())
reference = real_appn
candidate = result

# cider_score = {}
# score, scores = Cider().compute_score(reference, candidate)
# cider_score["CIDEr"] = score

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
r_score = scorer.score(real_caption,result_final)
  
score = sentence_bleu(reference, candidate)
meteor = meteor_score(reference, candidate)
print(f"METEOR score: {meteor*100}")
print(f"ROUGE-L score: {r_score}")
print(f"BLEU score: {score*100}")
# print(f"CIDEr score: {cider_score}")

print('BLEU-1 score: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('BLEU-2 score: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('BLEU-3 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('BLEU-4 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))

print ('Real Caption:', real_caption)
print ('Prediction Caption:', result_final)
plot_attention(image, result, attention_plot)
print(f"time took to Predict: {round(time.time()-start)} sec")

Image.open(img_name_val[rid])

to_lang = input("Enter a language to translate the caption : ")
print(to_lang)

translator= Translator(to_lang=to_lang)
translation = translator.translate(result_final)
print("Translated text : ")
print(translation)

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]

real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

# remove <start> and <end> from the real_caption
first = real_caption.split(' ', 1)[1]
real_caption = first.rsplit(' ', 1)[0]

#remove "<unk>" in result
for i in result:
   if i=="<unk>":
       result.remove(i)

for i in real_caption:
   if i=="<unk>":
       real_caption.remove(i)

#remove <end> from result        
result_join = ' '.join(result)
result_final = result_join.rsplit(' ', 1)[0]

real_appn = []
real_appn.append(real_caption.split())
reference = real_appn
candidate = result

# cider_score = {}
# score, scores = Cider().compute_score(reference, candidate)
# cider_score["CIDEr"] = score

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
r_score = scorer.score(real_caption,result_final)
  
score = sentence_bleu(reference, candidate)
meteor = meteor_score(reference, candidate)
print(f"METEOR score: {meteor*100}")
print(f"ROUGE-L score: {r_score}")
print(f"BLEU score: {score*100}")
# print(f"CIDEr score: {cider_score}")

print('BLEU-1 score: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('BLEU-2 score: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('BLEU-3 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('BLEU-4 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))

print ('Real Caption:', real_caption)
print ('Prediction Caption:', result_final)
plot_attention(image, result, attention_plot)
print(f"time took to Predict: {round(time.time()-start)} sec")

Image.open(img_name_val[rid])

to_lang = input("Enter a language to translate the caption : ")
print(to_lang)

translator= Translator(to_lang=to_lang)
translation = translator.translate(result_final)
print("Translated text : ")
print(translation)

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]

real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

# remove <start> and <end> from the real_caption
first = real_caption.split(' ', 1)[1]
real_caption = first.rsplit(' ', 1)[0]

#remove "<unk>" in result
for i in result:
   if i=="<unk>":
       result.remove(i)

for i in real_caption:
   if i=="<unk>":
       real_caption.remove(i)

#remove <end> from result        
result_join = ' '.join(result)
result_final = result_join.rsplit(' ', 1)[0]

real_appn = []
real_appn.append(real_caption.split())
reference = real_appn
candidate = result

# cider_score = {}
# score, scores = Cider().compute_score(reference, candidate)
# cider_score["CIDEr"] = score

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
r_score = scorer.score(real_caption,result_final)
  
score = sentence_bleu(reference, candidate)
meteor = meteor_score(reference, candidate)
print(f"METEOR score: {meteor*100}")
print(f"ROUGE-L score: {r_score}")
print(f"BLEU score: {score*100}")
# print(f"CIDEr score: {cider_score}")

print('BLEU-1 score: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('BLEU-2 score: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('BLEU-3 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('BLEU-4 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))

print ('Real Caption:', real_caption)
print ('Prediction Caption:', result_final)
plot_attention(image, result, attention_plot)
print(f"time took to Predict: {round(time.time()-start)} sec")

Image.open(img_name_val[rid])

to_lang = input("Enter a language to translate the caption : ")
print(to_lang)

translator= Translator(to_lang=to_lang)
translation = translator.translate(result_final)
print("Translated text : ")
print(translation)

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]

real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

# remove <start> and <end> from the real_caption
first = real_caption.split(' ', 1)[1]
real_caption = first.rsplit(' ', 1)[0]

#remove "<unk>" in result
for i in result:
   if i=="<unk>":
       result.remove(i)

for i in real_caption:
   if i=="<unk>":
       real_caption.remove(i)

#remove <end> from result        
result_join = ' '.join(result)
result_final = result_join.rsplit(' ', 1)[0]

real_appn = []
real_appn.append(real_caption.split())
reference = real_appn
candidate = result

# cider_score = {}
# score, scores = Cider().compute_score(reference, candidate)
# cider_score["CIDEr"] = score

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
r_score = scorer.score(real_caption,result_final)
  
score = sentence_bleu(reference, candidate)
meteor = meteor_score(reference, candidate)
print(f"METEOR score: {meteor*100}")
print(f"ROUGE-L score: {r_score}")
print(f"BLEU score: {score*100}")
# print(f"CIDEr score: {cider_score}")

print('BLEU-1 score: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('BLEU-2 score: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('BLEU-3 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('BLEU-4 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))

print ('Real Caption:', real_caption)
print ('Prediction Caption:', result_final)
plot_attention(image, result, attention_plot)
print(f"time took to Predict: {round(time.time()-start)} sec")

Image.open(img_name_val[rid])

to_lang = input("Enter a language to translate the caption : ")
print(to_lang)

translator= Translator(to_lang=to_lang)
translation = translator.translate(result_final)
print("Translated text : ")
print(translation)

rid = np.random.randint(0, len(img_name_val))
image = img_name_val[rid]

real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])
result, attention_plot = evaluate(image)

# remove <start> and <end> from the real_caption
first = real_caption.split(' ', 1)[1]
real_caption = first.rsplit(' ', 1)[0]

#remove "<unk>" in result
for i in result:
   if i=="<unk>":
       result.remove(i)

for i in real_caption:
   if i=="<unk>":
       real_caption.remove(i)

#remove <end> from result        
result_join = ' '.join(result)
result_final = result_join.rsplit(' ', 1)[0]

real_appn = []
real_appn.append(real_caption.split())
reference = real_appn
candidate = result

# cider_score = {}
# score, scores = Cider().compute_score(reference, candidate)
# cider_score["CIDEr"] = score

scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
r_score = scorer.score(real_caption,result_final)
  
score = sentence_bleu(reference, candidate)
meteor = meteor_score(reference, candidate)
print(f"METEOR score: {meteor*100}")
print(f"ROUGE-L score: {r_score}")
print(f"BLEU score: {score*100}")
# print(f"CIDEr score: {cider_score}")

print('BLEU-1 score: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))
print('BLEU-2 score: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))
print('BLEU-3 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))
print('BLEU-4 score: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))

print ('Real Caption:', real_caption)
print ('Prediction Caption:', result_final)
plot_attention(image, result, attention_plot)
print(f"time took to Predict: {round(time.time()-start)} sec")

Image.open(img_name_val[rid])

to_lang = input("Enter a language to translate the caption : ")
print(to_lang)

translator= Translator(to_lang=to_lang)
translation = translator.translate(result_final)
print("Translated text : ")
print(translation)